---
layout: post
title: Learning JAX, Flax and comparing it to PyTorch
data: 2024-03-27 09:00:00
description: 
tags: ML code JAX Flax Pytorch
categories: Programming
published: false
related_posts: false
---

Looking at UVA deep learning tutorials in JAX and learning JAX, along with new techniques in deep learning.

### Jax Intro

JAX compiles the functions and numerical operations for accelerators just in time, unlike PyTorch which uses dynamic computation graphs. Knowing what operations that happen ahead in the layers of a network helps compilation, fusion and optimization of the execution of operations in the network. This JIT compilation comes with a price such as constraints on how you write functions and handle variables without any side-effects, effecting random number generators. "JAX compiles the functions based on anticipated shaped of all arrays/tensors in the function". If the shape or the program flow of the tensors depend on the values of the tensor, you will have some problems.

Doing JAX tutorials: [JAX 101](https://jax.readthedocs.io/en/latest/jax-101/index.html) 

Code:

```python

import jax
import jax.numpy as jnp


```
