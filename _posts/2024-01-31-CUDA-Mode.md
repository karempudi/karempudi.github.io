---
layout: post
title: Learning CUDA, Triton, and related things
data: 2024-01-15 09:00:00
description: 
tags: ML code CUDA
categories: Programming
published: false
related_posts: false
---

By the end of this lecture series, I want to write a really fast spatial clustering algorithms with certain priors on the structure of the spatial domain. I will use this later on in my work.

Goals: I would be learning CUDA, Triton, pytorch internals and much more related to running modern ML/AI algorithms faster. I should be able to understand FlashAttention, MAMBA, tinygrad etc and all the new optimizations and interesting things people are doing these days.

This is just my notes all in one place. The original repo for the code from the CUDA-MODE lectures is [here](https://github.com/cuda-mode/lectures.git).

### [Lecture 1](https://www.youtube.com/watch?v=LuhJEEJQgUM)

#### Easiest way to 
Easiest way to run the get C++ functions access is to use `load_inline` function from `torch.utils.cpp_extension`. The code is simple, it generates a shared `.so` file. The functions are loaded and accessible in python. 

```py
import torch
import time
from time import perf_counter
from torch.utils.cpp_extension import load_inline

start = perf_counter()

cpp_source = """
std::string hello_world() {
  return "Hello World!";
}
std::string hello_praneeth() {
  return "Hello Praneeth!";
}
"""

my_module = load_inline(
    name='my_module',
    cpp_sources=[cpp_source],
    functions=['hello_world', 'hello_praneeth'],
    verbose=False,
    build_directory='./tmp'
)
print(f"Time to compile and module: {perf_counter() - start}s")

start_functions = perf_counter()
print(my_module.hello_world())
print(my_module.hello_praneeth())
print(f"Time to run two functions: {perf_counter() - start_functions}")
```

The compilation for the first time took 11s, but reloading only took 6ms. This is nice for simple functions. It generates a .cpp file and defines a `PYBIND11_MODULE` with the functions defined as part of the module.


#### Profiling pytorch code.

Using autograd profiler (will be deprecated in the future)

``` py
with 

```



### [Lecture 2]()

Reading Programming massively parallel programming Chapters 1,2,3 and doing exercises.

Device means GPUs, host means CPU. Kernel is the function that run on the device. Threads run the function and operate on the data sitting in memory of the GPU. Each thread operates independently and does a small computation. Lauching threads on GPUs is much easier and faster than CPU threads as no scheduling and swapping threads happens. People launch thousands of thread on the GPU and they are grouped into blocks.


Simplest program is to add two vectors. A simple program has few parts


Simple program in C or C++ looks like this.

``` c
#include <stdio.h>
#include <stdlib.h>
#include <time.h>

void vecAdd(float* A_h, float* B_h, float* C_h, int n) {
    for (int i = 0; i < n; i++){
        C_h[i] = A_h[i] + B_h[i];
    }
}

int main() {
    
    int N = 10;
    srand(time(0));
    float *A, *B, *C;
    A = (float*)malloc(N * sizeof(float));
    B = (float*)malloc(N * sizeof(float));
    C = (float*)malloc(N * sizeof(float));

    for (int i = 0; i < N; i++){
        A[i] = (float)rand() / RAND_MAX;
        B[i] = (float)rand() / RAND_MAX;
        C[i] = 0;
    }

    vecAdd(A, B, C, N);
    for (int i = 0; i < N; i++) {
        printf("A[%d] + B[%d] = C[%d]\n", i, i, i);
        printf("%.4f + %.4f = %.4f\n", A[i], B[i], C[i]);
    }
    free(A);
    free(B);
    free(C);
    return 0;
}
```

  1. Allocate memory of a the arrays/ data structures used on the device's memory and copy intialized data onto device.
  2. Launch kernel to operate.
  3. Copy data back and free memory on the GPU

Memory on the GPU is called Global memory.

Same vector add function from above would look like this in CUDA.

```Cuda

```


### [Lecture 3]()


### [Lecture 4]()


### [Lecture 5]()


